<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Roberto Yus</title>
    <link>/project/</link>
      <atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 23 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/logo.png</url>
      <title>Projects</title>
      <link>/project/</link>
    </image>
    
    <item>
      <title>Android goes semantic</title>
      <link>/project/androidgoessemantic/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/project/androidgoessemantic/</guid>
      <description>&lt;p&gt;The massive spread of mobile computing in our daily lives has attracted a huge community of mobile apps developers. These developers can take advantage of the benefits of semantic technologies (such as knowledge sharing and reusing, knowledge decoupling, etc.) to enhance their applications. Moreover, the use of semantic reasoners would enable them to create more intelligent applications capable of inferring logical consequences from the knowledge considered. However, using semantic APIs and reasoners on current Android-based devices is not problem-free and, currently, there are no remarkable efforts to enable mobile devices with semantic reasoning capabilities.&lt;/p&gt;
&lt;p&gt;In our work, we analyze whether the most popular current available DL reasoners can be used on Android-based devices. We evaluate the efforts needed to port them to the Android platform, taking into account its limitations, and present some tests to show the performance of these reasoners on current smartphones/tablets.&lt;/p&gt;
&lt;h4 id=&#34;useful-links-and-references&#34;&gt;Useful links and references&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>FaceBlock</title>
      <link>/project/faceblock/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/project/faceblock/</guid>
      <description>&lt;p&gt;Wearable technology is changing the human-computer interaction paradigm by improving the user experience. For example, eyewear technology, such as Google Glass, is opening a new world of possibilities. One of the best capabilities of such devices is that they allow spontaneous and effortless photo taking. Instead of pulling out your camera, turning it on, aiming, taking a picture and putting it away, Google Glass makes it as easy as saying “Okay Glass, take a picture”. However, their strength has also become controversial: people worry that their pictures could be taken without their knowledge or consent, violating their privacy. These privacy concerns are not new and have existed since camera technology became accessible to everyone. But, Google Glass makes it difficult to know when someone is taking a picture of you. Resorting to drastic measures, like banning their use in public places, is not a good solution.&lt;/p&gt;
&lt;p&gt;We present FaceBlock, which protects the privacy rights of individuals by allowing them to choose whether or not to be included in pictures taken with Glass. Our approach allows a user’s mobile device to share privacy policies with nearby Glass devices using a P2P communication channel (our prototype uses Bluetooth). Along with the policy, users share information that helps identify them in a picture. When the Google Glass takes a picture
the system recognizes faces on it, checks policies it has received, and obscures faces as necessary.&lt;/p&gt;
&lt;h4 id=&#34;useful-links-and-references&#34;&gt;Useful links and references&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;FaceBlock in the news:
&lt;ul&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;Nature: &amp;ldquo;What could derail the wearables revolution?&amp;rdquo;, 2015&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;eBiquity UMBC: &amp;ldquo;Do not be a Gl***hole, use Face-Block.me!&amp;rdquo;, 2014&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Infoboxer</title>
      <link>/project/infoboxer/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/project/infoboxer/</guid>
      <description>&lt;p&gt;Wikipedia infoboxes summarize the most important information of a Wikipedia page. In addition to being useful for Wikipedia readers, knowledge bases like DBpedia and Google Knowledge Graph can easily process and use their semi-structured content. However, the current mechanisms for creating or extending infoboxes are difficult and complex for editors to use. The result is fewer Wikipedia pages with infoboxes and more errors, inaccuracies, and deficiencies in existing infoboxes.&lt;/p&gt;
&lt;p&gt;We present an approach to help users create rich and accurate Wikipedia infoboxes. Our system uses statistical and semantic knowledge from linked data sources to simplify the process of creating and extending infobox content. It constructs semantic infobox templates by suggesting common attributes and their values from similar articles and controlling the expected values semantically. We present the implemented prototype and experiments with users that evaluate both the quality of the new infobox content and the users&#39; subjective experience.&lt;/p&gt;
&lt;h4 id=&#34;useful-links-and-references&#34;&gt;Useful links and references&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>MultiCAMBA</title>
      <link>/project/multicamba/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/project/multicamba/</guid>
      <description>&lt;p&gt;For a Technical Director (TD) in charge of a live broadcasting, selecting the best camera shots among the available video sources is a challenging task, even more now that the number of cameras (some of them mobile, or attached to moving objects) in the broadcasting of sport events is increasing. So, the TD needs to manage a great amount of continuously changing information to quickly select the camera whose view should be broadcasted. Besides, the better the decisions made by the TD, the more interesting the content for the audience. Therefore, the development of systems that could help the TD with the selection of camera views is demanded by broadcasting organizations.&lt;/p&gt;
&lt;p&gt;In this project, we present the system MultiCAMBA that helps TDs in the live broadcasting task by allowing them to indicate in run-time their interest in certain kind of shots, and the system will show the cameras that are able to provide them. To achieve this task, the system manages location-dependent queries generated according to the interests of the TD. Moreover, to avoid the use of costly on line real-image processing techniques over the camera views, such real camera views are recreated in a 3D engine by using the information contained in a 3D model of the scenario. This model is updated continuously with real-time data retrieved from the real objects and cameras in the scenario. In this way, the system extracts high-level semantic features of 2D projections of the 3D reconstruction of the camera views.&lt;/p&gt;
&lt;h4 id=&#34;useful-links-and-references&#34;&gt;Useful links and references&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>SemIoTic</title>
      <link>/project/semiotic/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/project/semiotic/</guid>
      <description>&lt;p&gt;SemIoTic is a middleware framework for IoT smart spaces that provides application developers and end-users with a semantic domain-relevant view of a smart space
while hiding the complexity of having to deal with/understand lower-level information generated by sensors and actuators. SemIoTic uses a metamodel based on the popular
SOSA/SSN ontology with some extensions to represent relationships between the low-level IoT devices&#39; world (i.e., devices, observations) and semantic concepts
(i.e., observable attributes of users and spaces). It supports a language where users can express their action requirements (i.e., requests for sensor data, commands
for actuators, and privacy preferences) in terms of user-friendly high-level concepts. We present an ontology-based algorithmic approach to translate user-defined actions
into sensor/actuators commands. Finally, our end-to-end approach includes a cross-layer solution to provide interoperability with diverse IoT devices and their data exchange protocols.&lt;/p&gt;
&lt;p&gt;Several instances of SemIoTic have been deployed and used in IoT spaces. At the Donald Bren Hall building of UC Irvine, building occupants
have access to Concierge and Building Analytics smart applications and building administrators have access to a Contact Tracing Application to help combat the spread of COVID-19.
Also, SemIoTic has been deployed inside a navy ship from the Naval Information Warfare Center (NIWC)
in the context of the Trident Warrior exercise 2019/2020. SemIoTic enabled testing of applications such as occupancy analysis in multiple
regions of the ship.&lt;/p&gt;
&lt;h4 id=&#34;useful-links-and-references&#34;&gt;Useful links and references&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;SemIoTic is developed as part of the &lt;!-- raw HTML omitted --&gt;TIPPERS&lt;!-- raw HTML omitted --&gt; (Testbed for IoT-based Privacy-Preserving PERvasive Spaces) DARPA research project.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SHERLOCK</title>
      <link>/project/sherlock/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/project/sherlock/</guid>
      <description>&lt;p&gt;Nowadays people are exposed to huge amounts of information that are generated continuously; the ever-increasing use of mobile devices and their pervasiveness enable users to receive and create new contents almost anywhere. Currently, Location-Based Services (LBSs) are very popular, as they offer users customized information taking into account their current location. However, whereas users have access to a wide variety of information, current mobile apps, Web pages, and LBSs are designed for specific scenarios and goals. So, the information about the context they manage is not explicitly represented but embedded in their code. Moreover, developing services ad hoc for specific purposes leads to the fact that there exist thousands of them (even with the same purpose), and therefore it is difficult to choose the most suitable one. Therefore, currently it is a challenge to provide a common framework that allows to manage knowledge obtained from data sent by heterogeneous moving objects (textual data, multimedia data, sensor data, etc.). In addition, the challenge is even greater considering situations where the system must adapt itself to contexts where the knowledge changes dynamically and in which moving objects can use different underlying wireless technologies and positioning systems.&lt;/p&gt;
&lt;p&gt;We propose the system SHERLOCK, which offers a common framework with new functionalities for LBSs. As its namesake, the well-known Arthur Conan Doyle&amp;rsquo;s character, SHERLOCK uses abductive and deductive reasoning to infer information to answer user information requests (e.g., by providing interesting LBSs). Our system searches and shares up-to-date knowledge from nearby devices to relieve the user from knowing and managing such knowledge directly. Besides, the system guides the user in the process of selecting the service that best fits his/her needs in the given context. Finally, SHERLOCK processes user requests continuously to provide up-to-date answers in heterogeneous and dynamic contexts. Ontology reasoning and alignment methods are used to represent and manage, in a distributed way, the knowledge that describes moving and static objects (e.g., persons, vehicles, etc.) and interesting areas in a scenario. Moreover, the system uses mobile agent technology to carry the processing tasks wherever necessary in the dynamic underlying networks at any time.&lt;/p&gt;
&lt;h4 id=&#34;useful-links-and-references&#34;&gt;Useful links and references&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>TIPPERS</title>
      <link>/project/tippers/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/project/tippers/</guid>
      <description>&lt;p&gt;TIPPERS is an IoT data management middleware that manages IoT smart spaces by collecting sensor data, inferring semantically meaningful information from it, and offering such inferences to developers to create smart applications. Sensor data collection, processing, and sharing leads to potential violations of people’s privacy given the sensitivity of the data and the inferences that can be extracted from it. TIPPERS integrates different Privacy Enhancing Technologies (e.g., policy-based access control, differential privacy, secure computing, etc.) to deal with privacy issues in IoT data management.&lt;/p&gt;
&lt;p&gt;TIPPERS has been deployed in several IoT smart spaces. The main deployment is at the UC Irvine campus where it handles IoT data from more than 20 buildings. Based on such data (including connectivity logs from WiFi APs, HVAC data, video footage, etc.) TIPPERS generates inferences such as location of people and occupancy of spaces. We have developed several smart applications which use those inferences to offer personalized services to the campus inhabitants. In particular, we have recently developed COVID-19 regulation monitoring tools to assess the adherence of the community to occupancy regulations. TIPPERS has also been successfully deployed in two US Navy ships as part of the Trident Warrior 2019 and 2020 excersice.&lt;/p&gt;
&lt;h4 id=&#34;useful-links-and-references&#34;&gt;Useful links and references&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;TIPPERS is funded by DARPA as part of the Brandeis program.&lt;/li&gt;
&lt;li&gt;TIPPERS in the news:
&lt;ul&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;The Washington Post: &amp;ldquo;The Technology 202: Tech to contain coronavirus on college campuses sparks fresh privacy concerns&amp;rdquo;, 2020&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;Health IT Analytics: &amp;ldquo;UCI Uses Campus Wi-Fi to Test COVID-19 Contact Tracing App&amp;rdquo;, 2020&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;UCI news: &amp;ldquo;Test bed for COVID-19 contact tracing system&amp;rdquo;, 2020&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;Defense Visual Information Distribution Service: &amp;ldquo;NAVWAR Assesses New Tracking Technology for COVID-19 Mitigation&amp;rdquo;, 2020&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;Military News: &amp;ldquo;Navy tests experimental technology at Trident Warrior 2019&amp;rdquo;, 2019&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;UCI CS News: &amp;ldquo;IoT Hackathon Apps Coming Soon&amp;rdquo;, 2018&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;UCI CS News: &amp;ldquo;ICS launches IoT Hackathon to discover student talent who can develop a UCI ‘smart’ campus&amp;rdquo;, 2017&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
